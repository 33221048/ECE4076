{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "data = torchvision.datasets.CIFAR10(root='./data/',transform=transform)\n",
    "train_loader = DataLoader(data,batch_size=8,shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning image representations using compression.\n",
    "\n",
    "Throughout this unit, we have pushed this idea that we need to find representations of images (or image patches or regions) that satisfy certain properties, are invariant to various conditions we may encounter, or generally useful for some downstream task. The supervised learning methods we have encountered specifically train the model to learn representations for the latter.\n",
    "\n",
    "We now move to unsupervised approaches to representation learning. We'll start with an autoencoder. The idea is simple, we first squash our image into a latent vector, and then try to reconstruct the original image from this reduced representation. Remember how we used PCA for this before? We're going to do the same thing, but replace the linear projection with a non-linear function, parameterised by a neural network.\n",
    "\n",
    "You will probably need a decent GPU to run the code below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start by building an encoder model\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(nn.Conv2d(channels,32,kernel_size=4,stride=2,padding=1),\n",
    "                                     nn.LeakyReLU(),\n",
    "                                     nn.Conv2d(32,64,kernel_size=4,stride=2,padding=1),\n",
    "                                     nn.LeakyReLU(),\n",
    "                                     nn.Conv2d(64,128,kernel_size=4,stride=2,padding=1),\n",
    "                                     nn.LeakyReLU(),\n",
    "                                     nn.Flatten())\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        return self.encoder(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = Encoder()(torch.randn(1,3,32,32)).shape[1]\n",
    "print('Our encoder takes in a 32x32 dimensional image and outputs a feature vector of dimension:',latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will build a decoder model\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,latent_dim=2048):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.Sequential(nn.Unflatten(dim=-1,unflattened_size=(128,4,4)),\n",
    "                                     nn.ConvTranspose2d(128,64,kernel_size=4,stride=2,padding=1), #See https://d2l.ai/chapter_computer-vision/transposed-conv.html\n",
    "                                     nn.LeakyReLU(),\n",
    "                                     nn.ConvTranspose2d(64,32,kernel_size=4,stride=2,padding=1),\n",
    "                                     nn.LeakyReLU(),\n",
    "                                     nn.ConvTranspose2d(32,3,kernel_size=4,stride=2,padding=1),\n",
    "                                     nn.Sigmoid())\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        return self.decoder(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our decoder takes in %d dimensional latent and generates an image of dimension: %s'%(latent_dim,Decoder(latent_dim=latent_dim)(torch.randn(1,latent_dim)).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity:\n",
    "\n",
    "The convolution parameters were carefully chosen to make these dimensions add up. Striding is used to both shrink and expand images. Try to change the architecture to take in 64x64 dim images and generate 64x64 images. Think about the convolution response size equations we looked at earlier in the unit - how could you avoid doing this by trial and error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder(latent_dim=latent_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's train our auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = None  # exponential moving average\n",
    "n_epochs = 50\n",
    "\n",
    "optim = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),lr=1e-4)\n",
    "\n",
    "# with tqdm.tqdm(total=n_epochs) as pbar:\n",
    "    \n",
    "for step in range(n_epochs):\n",
    "\n",
    "    t = tqdm(total=len(train_loader))\n",
    "\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        im,_ = batch\n",
    "        im = im.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        latent = encoder(im)\n",
    "        im_recon = decoder(latent)\n",
    "        loss = torch.sum(0.5*(im_recon-im)**2)/im.shape[0]\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # Some visualisation during training.\n",
    "        t.update(1)\n",
    "        \n",
    "        if avg_loss is None:\n",
    "            avg_loss = loss.item()\n",
    "        else:\n",
    "            avg_loss = 0.95*avg_loss + 0.05*loss.item()\n",
    "        t.set_description(f\"Iter: {step}. Average Loss: {avg_loss:.04f}\")\n",
    "    t.reset()\n",
    "        \n",
    "    plt.clf()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(im[0,:,:,:].transpose(0,2).cpu())\n",
    "    plt.title('Original image')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(im_recon[0,:,:,:].transpose(0,2).detach().cpu())\n",
    "    plt.title('Reconstructed image')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, the autoencoder learns to compress our images and then reconstruct the original images. We could use these representations as features to match images, or train a classifier using the features directly.\n",
    "\n",
    "Lets use a technique called [t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) to reduce the dimensionality of these latent features even further, and see whether the representations captured by our model learned anything interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use encoder to extract feature vectors for all images\n",
    "feats = []\n",
    "labs = []\n",
    "for i,batch in enumerate(train_loader):\n",
    "    im,l = batch\n",
    "    latent = encoder(im.to(device)) #to save time, I'll only take the first sample in each batch\n",
    "    labs.append(l[0])\n",
    "    feats.append(latent[0,:])\n",
    "    \n",
    "labs = torch.stack(labs).numpy()\n",
    "feats = torch.vstack(feats).detach().cpu().numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "feature = PCA(n_components=50).fit_transform(feats)\n",
    "tsne_feat = TSNE(n_components=2,verbose=1).fit_transform(feature)\n",
    "\n",
    "for i in range(10):\n",
    "    plt.scatter(tsne_feat[labs == i, 0], tsne_feat[labs == i, 1])\n",
    "plt.legend(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, I don't see a pattern in there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoding\n",
    "\n",
    "The autoencoder learns a deterministic transform for a single image. Let's extend it to make it a probabalistic generative model, using [variational autoencoding (VAE)](https://arxiv.org/abs/1312.6114). We've already encountered one of these before with our Gaussian mixture model\n",
    "$$p(x) = \\sum_{i=0}^N p(i) p(x|i)$$\n",
    "$$p(x) = \\sum_{i=0}^N \\pi_i \\mathcal{N}(x|\\mu_i,\\sigma_i)$$\n",
    "\n",
    "Here, we could consider the Gaussian indicator a latent variable, and the Gaussian a conditional dependent on the value of i. We can formulate a more general version of this - a *latent variable model* of our image $I$, conditioned on a latent variable or feature vector $x$.\n",
    "\n",
    "$$ p(I) = \\int p(I|x)p(x) dx $$\n",
    "\n",
    "If we want to fit this model to the space of images, we need some way to learn the distribution over images. *Variational* autoencoding provides us with an approach to do this. \n",
    "\n",
    "$$ p(I) = \\int p(I|x) p(x) dx $$\n",
    "Here, $p(I|x)$ is our decoder.\n",
    "\n",
    "$$ \\log p(I) = \\log \\int p(I|x) p(x) dz $$\n",
    "\n",
    "Lets introduce an encoder model $q(I|x)$ into this equation:\n",
    "$$ \\log p(I) = \\log \\int q(x|I) \\frac{p(I|x) p(x)}{q(x|I)} dx $$\n",
    "\n",
    "\n",
    "$$ \\log p(I) \\ge \\int q(x|I) \\log \\frac{p(I|x) p(x)}{q(x|I)} dx \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, \\text{See [Jensen] for this inequality} $$\n",
    "\n",
    "$$ = \\int q(x|I) \\log p(I|x) dz  - \\int q(x|I) \\log \\frac{q(x|I)}{p(x)} dx$$\n",
    "$$ = \\mathbb{E}_q [\\log p(I|x)]  - \\mathcal{KL}(q(x|I)||p(x))$$\n",
    "\n",
    "We call this the evidence lower bound. This objective is cool, the first term is effectively a reconstruction quality term like we had in our original autoencoder. The second term is a divergence measure that pulls the distribution over the latent space to look like the prior $p(x)$ How do we choose the prior? Let's assume its a zero mean Gaussian $\\mathcal{N}(0,1)$. Likewise, we will choose our $q(x|I)$ to be a Gaussian, using neural networks to predict the mean and variance of this. We sometimes call this a mean field approximation.  We often include a $\\beta$ scaling parameter to better balance these losses - leading to [$\\beta$-VAE](https://openreview.net/forum?id=Sy2fzU9gl).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self,latent_dim=128,device='cpu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder() \n",
    "        self.decoder = Decoder(latent_dim=latent_dim)\n",
    "        \n",
    "        # These layers project our encoded embedding to form the q(x|I) = N(mu,lsig)\n",
    "        self.mu = nn.Linear(latent_dim,latent_dim)\n",
    "        self.log_var = nn.Linear(latent_dim,latent_dim) # We use the log var to avoid numerical issues\n",
    "        \n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = device\n",
    "        \n",
    "        \n",
    "    def reparametrise(self,mu,lvar): # This reparametrisation trick draws a sample from a Gaussian (remember week 7?)\n",
    "        \n",
    "        return mu + lvar.mul(0.5).exp()*torch.randn_like(mu).to(self.device) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        z = self.encoder(x) \n",
    "        mu = self.mu(z)\n",
    "        lvar = self.log_var(z)\n",
    "        \n",
    "        zs = self.reparametrise(mu,lvar) # Draw a sample\n",
    "        \n",
    "        x_recon = self.decoder(zs) # Reconstruct\n",
    "        \n",
    "        return x_recon, mu, lvar, zs\n",
    "    \n",
    "    def sample(self,Nsamples=1):\n",
    "        \n",
    "        z = torch.randn(Nsamples,self.latent_dim).to(self.device)\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    \n",
    "    def loss_fn(self,x,beta=1e-2): #Copmpute full loss\n",
    "        \n",
    "        x_recon, mu, lvar, _ = self.forward(x)\n",
    "        \n",
    "        # Note the sign here, we are minimising this loss, so reducing the recon error and the divergence between q and p\n",
    "        return torch.sum((x_recon-x)**2)/x.shape[0], -beta*0.5*torch.sum(1 + lvar - mu**2 -  lvar.exp())/x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(device=device,latent_dim=latent_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_recon_loss = None  # exponential moving average\n",
    "avg_kl_loss = None  # exponential moving average\n",
    "n_epochs = 50\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "\n",
    "# with tqdm.tqdm(total=n_epochs) as pbar:\n",
    "    \n",
    "for step in range(n_epochs):\n",
    "\n",
    "    t = tqdm(total=len(train_loader))\n",
    "\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        im,_ = batch\n",
    "        \n",
    "        im = im.to(device)\n",
    "        optim.zero_grad()\n",
    "        recon_loss,kl_loss = model.loss_fn(im,beta=0.1)\n",
    "        loss = recon_loss+kl_loss\n",
    "         # I am training with a relatively high learning rate so I can show this in class, this clips my gradients so they don't explode\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # Some visualisation during training.\n",
    "        t.update(1)\n",
    "        \n",
    "        if avg_recon_loss is None:\n",
    "            avg_recon_loss = recon_loss.item()\n",
    "        else:\n",
    "            avg_recon_loss = 0.95*avg_recon_loss + 0.05*recon_loss.item()\n",
    "            \n",
    "        if avg_kl_loss is None:\n",
    "            avg_kl_loss = kl_loss.item()\n",
    "        else:\n",
    "            avg_kl_loss = 0.95*avg_kl_loss + 0.05*kl_loss.item()\n",
    "        t.set_description(f\"Iter: {step}. Average Recon Loss: {avg_recon_loss:.04f} Average KL Loss: {avg_kl_loss:.04f}\")\n",
    "    t.reset()\n",
    "    \n",
    "    im_recon,_,_,_ = model(im)\n",
    "        \n",
    "    plt.clf()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(im[0,:,:,:].transpose(0,2).cpu())\n",
    "    plt.title('Original image')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(im_recon[0,:,:,:].transpose(0,2).detach().cpu())\n",
    "    plt.title('Reconstructed image')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happened to the latent space of our VAE. We trained with a very small KL term, did it do anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use encoder to extract feature vectors for all images\n",
    "feats = []\n",
    "labs = []\n",
    "for i,batch in enumerate(train_loader):\n",
    "    im,l = batch\n",
    "    _,_,_,latent = model(im.to(device)) #to save time, I'll only take the first sample in each batch\n",
    "    labs.append(l[0])\n",
    "    feats.append(latent[0,:])\n",
    "    \n",
    "labs = torch.stack(labs).numpy()\n",
    "feats = torch.vstack(feats).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "feature = PCA(n_components=50).fit_transform(feats)\n",
    "tsne_feat = TSNE(n_components=2,verbose=1).fit_transform(feature)\n",
    "\n",
    "for i in range(10):\n",
    "    plt.scatter(tsne_feat[labs == i, 0], tsne_feat[labs == i, 1])\n",
    "plt.legend(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The space is now much more structured. Even though TSNE is a projection of our high dimensional feature vector, everything lies in a nice compact space. Play around with the beta term in the VAE and see what effect it has on image quality/ latent space. You may also wanting to try adjusting the latent space dimension and seeing how this changes the latet space - do you see any clustering of similar class images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that was a lot of maths. What did we gain? Well, first of all we got a generative model for free! Since we know that our images are squashed into a space that should look Gaussian, we can draw a sample from this distribution to decode. Lets look at some random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ims = model.sample(Nsamples=25).detach().cpu()\n",
    "grid_im = torchvision.utils.make_grid(Ims, nrow=5).transpose(0,2)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(grid_im)\n",
    "plt.title('Generated images')\n",
    "train_loader = DataLoader(data,batch_size=25,shuffle=True)\n",
    "ims,_ = next(iter(train_loader))\n",
    "grid_im = torchvision.utils.make_grid(ims, nrow=5).transpose(0,2)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(grid_im)\n",
    "plt.title('Real images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. These look like they could be synthetic images. With the right architecture and some loss tweaks, this is a very powerful generative modelling technique. We rely on this heavily in my (Michael) own research on latent dynamics models for robotics. \n",
    "\n",
    "Unfortunately, these images still look quite blurry and don't really resemble anything usedul. Until recently, these kind of probabilistic generative models had fallen out of favour, with generative adversarial networks producing much higher quality images. As we'll see in another notebook, diffusion models have thankfully made probabilistic generative models great again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
