{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython import display\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Use GPU if you have one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diffusion models\n",
    "\n",
    "We want to learn a generative model or distribution over data $x$. In the context of robotics this may be a behaviour cloning policy over some control actions, a human behaviour model, or a dynamics/ reward model. In the image generation case, this is a model that can produce realistic looking images.\n",
    "\n",
    "Assume we have a set of samples from a real data distribution:\n",
    "\n",
    "$$x_0 \\sim q(x)$$\n",
    "\n",
    "\n",
    "How can we approximate this distribution? We already have a few ways. Implicit generative modelling approaches like GANs learn to generate synthetic samples that look like real-world samples, by learning to confuse a classifier that is learning to tell these apart. If this sounds clunky it's because it is, and ideally we'd love a more direct, likelihood-based approach. Flows (invertible transforms of distributions) and variational autoencoding (fitting a latent variable model to maximise a variational lower bound) fall into this category, with nice principled justifications and training mechanisms, but unfortunately have never quite performed as well as GANs at high dimensional generative modelling. Diffusion models [(Sohl-Dickstein et al. 2015)](https://arxiv.org/abs/1503.03585) remedy this, providing excellent generative modelling performance, and an explicit likelihood based probabilistic formulation.\n",
    "\n",
    "#### Forward process\n",
    "\n",
    "Let's imagine a foward process, where we repeatedly add Gaussian noise to our original sample $x_0$, over a series of $T$ steps, to produce a sequence of noisy samples $x_1,\\ldots,x_T$. We'll do this with a *diffusion process*\n",
    "\n",
    "$$q (x_t|x_{t-1}) = \\mathcal{N}(x_t|\\sqrt{1-\\beta_t}x_{t-1},\\beta_t)$$\n",
    "\n",
    "This has a joint distribution\n",
    "\n",
    "$$q(x_{1:T}|x_0) = \\prod_{t=1}^{T} q (x_t|x_{t-1}) $$\n",
    "\n",
    "These steps are controlled by a variance schedule $\\{\\beta_t \\in (0,1) \\}^T_{t=1}$. As more steps are taken, the sample $x_0$ loses it's distinquishing structure until it is eventually overwhelmed by noise, and follows a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some test data\n",
    "N = 1000\n",
    "plt.figure(figsize=(15,15))\n",
    "X = make_swiss_roll(n_samples=N, noise=1e-1)[0][:, [0, 2]] / 10.0\n",
    "plt.subplot(5,5,1)\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.grid()\n",
    "\n",
    "# Slowly add noise\n",
    "Xhat = X\n",
    "steps = 1000\n",
    "for j in range(1,25):\n",
    "    Xhat = np.sqrt(1-j/steps)*Xhat + np.sqrt(j/steps)*np.random.randn(X.shape[0],X.shape[1])\n",
    "    plt.subplot(5,5,j+1)\n",
    "    plt.title('Beta: %2.2f'%np.sqrt(j/steps))\n",
    "    plt.scatter(Xhat[:,0],Xhat[:,1],alpha=0.1)\n",
    "    plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular process has a nice property. We can sample an $x_t$ at any point in time in closed form using a reparametrisation trick.\n",
    "\n",
    "Let $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha} = \\prod_{i=0}^t \\alpha_i$\n",
    "\n",
    "then \n",
    "\n",
    "$$ x_t = \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1}\\, \\text{where}\\, \\epsilon_{t-1} \\sim \\mathcal{N}(0,1)$$ \n",
    "$$ x_t = \\sqrt{\\alpha_t\\alpha_{t-1}} x_{t-2} + \\sqrt{1-\\alpha_t\\alpha_{t-1}}\\bar{\\epsilon}_{t-2}\\, \\text{where}\\, \\bar{\\epsilon_{t-2}} = \\epsilon_{t-1}+\\epsilon_{t-2} \\sim \\mathcal{N}(0,1)$$ \n",
    "\n",
    "Here, we use the fact that merging two independent standard Gaussians $\\bar{\\epsilon_{t-2}} = \\epsilon_{t-1}+\\epsilon_{t-2}$, results in another Gaussian, $\\mathcal{N}(0,1)$. \n",
    "\n",
    "Continuing the above, we obtain:\n",
    "\n",
    "$$ x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\epsilon$$\n",
    "\n",
    "$$ q(x_t|x_0) = \\mathcal{N}(x_t|\\sqrt{\\bar{\\alpha}_t}x_0,1-\\bar{\\alpha_t})$$\n",
    "\n",
    "This process allows us to obtain a distribution over $x_t$ after t steps or repeated noise addition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse process\n",
    "\n",
    "Now, imagine that we could reverse this process $q(x_{t-1}|x_t)$, this would allow us to start with a random variable drawn from Gaussian noise, and eventually generate a sample $x_0$ from this.\n",
    "\n",
    "To figure out how to do this, we'll start by looking at a related distribution, conditioned on $x_0$. By Bayes rule:\n",
    "\n",
    "$$q(x_{t-1}|x_t,x_0) = q(x_{t}|x_{t-1},x_0)\\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}$$\n",
    "\n",
    "These are all Gaussians, and we know all these (see above), so by the product/division of exponents we can write this as:\n",
    "\n",
    "$$ q(x_{t-1}|x_t,x_0) \\propto \\exp (-\\frac{1}{2}( \\frac{(x_t-\\sqrt{\\alpha_t}x_{t-1})^2}{\\beta_t} + \\frac{(x_{t-1}-\\sqrt{\\bar{\\alpha}_{t-1}}x_{0})^2}{1-\\bar{\\alpha}_{t-1}} - \\frac{(x_{t}-\\sqrt{\\bar{\\alpha}_{t}}x_{0})^2}{1-\\bar{\\alpha}_{t}}))$$\n",
    "\n",
    "This is another Gaussian, and with some non-trivial rearranging, we find that we can write this reverse process as a Gaussian transition, with mean $\\mu(x_{t},x_0)$ and variance $\\tilde{\\beta}_t$.\n",
    "\n",
    "$$ p_\\theta(x_{t-1}|x_{t}) = \\mathcal{N}(x_{t-1}|\\mu(x_t,x_0),\\tilde{\\beta}_t) $$\n",
    "\n",
    "$$ \\tilde{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}\\beta_t $$\n",
    "\n",
    "$$ \\mu(x_{t},x_0) = \\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}x_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1-\\bar{\\alpha}_t}x_0$$\n",
    "\n",
    "But remember,\n",
    "\n",
    "$$ x_0 = \\frac{1}{\\sqrt{\\alpha_t}}(x_t-\\sqrt{1-\\bar{\\alpha}_t} \\epsilon_t) $$\n",
    "\n",
    "So \n",
    "\n",
    "$$\\mu_t = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}})\\epsilon_t$$\n",
    "\n",
    "which gives us the complete reverse process:\n",
    "\n",
    "$$ q(x_{t-1}|x_{t}) = \\mathcal{N}(x_{t-1}|\\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}})\\epsilon_t,\\frac{1 - \\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}\\beta_t)$$\n",
    "\n",
    "Most of the quantities here are known, except the noise term $\\epsilon_t$. So lets train a neural network to predict the noise.\n",
    "\n",
    "$$ p_\\theta(x_{t-1}|x_{t}) = \\mathcal{N}(x_{t-1}|\\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}})\\epsilon_\\theta(x_t,t),\\frac{1 - \\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}\\beta_t)$$\n",
    "\n",
    "\n",
    "The equation above gives us a training objective. Given a sample $x_0$, and a sampled $t \\sim U(0,T)$, we draw a starting noise $\\epsilon \\sim \\mathcal{N}(0,1)$. We compute an $x_t$ and and $x_{t-1}$ using the forward process, predict $\\epsilon_\\theta(x_t,t)$, then take gradient steps to minimise the quantity\n",
    "\n",
    "$$L_t = || x_{t-1} - \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}})\\epsilon_\\theta(x_t,t)||^2$$\n",
    "\n",
    "[Ho et al. (2020)](https://arxiv.org/abs/2006.11239) decided to ignore the weighting, and just minimise the following instead, which apparently works better:\n",
    "$$L_t = || \\epsilon - \\epsilon_\\theta(x_t,t)||^2$$\n",
    "\n",
    "\n",
    "So the general algorithm is, sample a random initial noise $\\epsilon$, sample a random $t$ to jump ahead to, compute the forward process to get the noisy $x_t$. Get a neural network to predict the initial noise sample, conditioned on $x_t,t$. Once our model is trained, we can just run the reverse process using the model to predict the noise to be added, and eventually return a sample.\n",
    "\n",
    "Sounds complex? All we're doing is repeatedly adding noise to data, and training a neural network to *undo* the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's implement this in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "\n",
    "    def __init__(self, n_steps, minval = 0.0001, maxval = 0.02):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_steps = n_steps\n",
    "\n",
    "        self.beta = torch.linspace(minval, maxval, n_steps) # Our variance schedule\n",
    "        self.alpha = 1 - self.beta \n",
    "        self.alpha_bar = torch.cumprod(self.alpha,dim=0) # cummulative product\n",
    "\n",
    "    def diffusion_loss(self, model, x0,device='cpu'):\n",
    "        \n",
    "        batch_size = x0.shape[0]\n",
    "\n",
    "        # Sample our initial noise\n",
    "        eps = torch.randn_like(x0).to(device)\n",
    "\n",
    "        # Sample a random time along the process\n",
    "        t = torch.randint(0, self.n_steps, (batch_size,)).to(device)\n",
    "\n",
    "        # compute the closed form noisy sample x_t after t time steps\n",
    "        if len(x0.shape)>2:\n",
    "            a_t = self.alpha_bar.to(device)[t][:, None,None,None]\n",
    "        else:\n",
    "            a_t = self.alpha_bar.to(device)[t][:, None]\n",
    "       \n",
    "        x_t = torch.sqrt(a_t)*x0 + torch.sqrt(1 - a_t)*eps\n",
    "           \n",
    "        # predict the noise added given time t and this noisy sample\n",
    "        eps_pred = model(x_t, t)\n",
    "\n",
    "        # Compute error between predicted noise and sampled noise\n",
    "        return nn.MSELoss()(eps_pred, eps)\n",
    "\n",
    "    #Reverse process\n",
    "    def sample(self, model, n_samples,d_in=2,x=None,plot=False,device='cpu'): \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # start off with an intial random set of samples\n",
    "            if x == None:\n",
    "                x = torch.randn(n_samples, d_in).to(device)\n",
    "\n",
    "            for t in reversed(range(self.n_steps)):\n",
    "                \n",
    "                if len(x.shape)>2:\n",
    "                    a = self.alpha[t].to(device).repeat(n_samples)[:, None,None,None]\n",
    "                    abar = self.alpha_bar[t].to(device).repeat(n_samples)[:, None,None,None]\n",
    "                    abar_1 = self.alpha_bar[t-1].to(device).repeat(n_samples)[:, None,None,None]\n",
    "                else:\n",
    "                    a = self.alpha[t].to(device).repeat(n_samples)[:, None]\n",
    "                    abar = self.alpha_bar[t].to(device).repeat(n_samples)[:, None]\n",
    "                    abar_1 = self.alpha_bar[t-1].to(device).repeat(n_samples)[:, None]\n",
    "\n",
    "                # Use learned model to predict noise to apply to samples\n",
    "                eps_theta = model(x, torch.tensor([t] * n_samples, dtype=torch.long).to(device))\n",
    "                \n",
    "                # Step process forward\n",
    "               \n",
    "                x_mean = 1/torch.sqrt(a)*(x - eps_theta*(1 - a)/torch.sqrt(1 - abar))\n",
    "                sigma_t = torch.sqrt(self.beta[t]).to(device)\n",
    "                \n",
    "                # Note, the derivation above used the following uncertainty for the forward process\n",
    "                # torch.sqrt((1-abar_1)/(1-abar)*(1-a))\n",
    "                # but Ho 2020, found just using beta = 1-a is fine\n",
    "\n",
    "                # sample a  new noise to add\n",
    "                z = torch.randn_like(x).to(device)\n",
    "                x = x_mean + sigma_t * z\n",
    "\n",
    "                if plot:\n",
    "                    plt.subplot(5,5,self.n_steps-t)\n",
    "                    plt.scatter(X[:,0],X[:,1])\n",
    "                    plt.quiver(x.detach().numpy()[:,0], x.detach().numpy()[:,1], -eps_theta[:,0].detach().numpy(),-eps_theta[:,1].detach().numpy())\n",
    "                    plt.grid()\n",
    "            model.train()\n",
    "            return x_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll use a simple FCN to predict the noise (in the Ho et al. paper they use a more complex U-net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in: int, d_out: int, d_hidden: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(nn.Linear(d_in+8,d_hidden),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(d_hidden,d_hidden),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(d_hidden,d_out))\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # We will include time in our network using sinusoidal encoding\n",
    "        time_encoding = (torch.hstack([torch.sin(t[:,None]*freq) for freq in [0, 2, 4, 8]]+[torch.cos(t[:,None]*freq) for freq in [0, 2, 4, 8]]))\n",
    "        return self.model(torch.hstack([x,time_encoding]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_hidden = 128\n",
    "d_in = d_out = X.shape[1]\n",
    "batch_size = 128\n",
    "n_epochs = 5000\n",
    "n_steps = 25\n",
    "\n",
    "# Make a noise prediction model, and diffusion process\n",
    "model = FCN(d_in=2, d_hidden=d_hidden, d_out=2)\n",
    "ddpm = DDPM(n_steps=n_steps)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's train and visualise the generated samples as the noise prediction model learns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "avg_loss = None  # exponential moving average\n",
    "\n",
    "with tqdm(total=n_epochs*(len(X)/batch_size)) as pbar:\n",
    "    \n",
    "    for _ in range(n_epochs):\n",
    "    \n",
    "        ids = np.random.choice(len(X), len(X)-batch_size, replace=False)\n",
    "    \n",
    "        for i in range(0, len(ids), batch_size):\n",
    "\n",
    "            x = torch.tensor(X[ids[i:i +batch_size]], dtype=torch.float32)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss = ddpm.diffusion_loss(model, x)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            # Some visualisation during training.\n",
    "            pbar.update(1)\n",
    "            \n",
    "            if avg_loss is None:\n",
    "                avg_loss = loss.item()\n",
    "            else:\n",
    "                avg_loss = 0.95*avg_loss + 0.05*loss.item()\n",
    "            if not step % 100:\n",
    "                pbar.set_description(f\"Iter: {step}. Average Loss: {avg_loss:.04f}\")\n",
    "                samples = ddpm.sample(model, n_samples=512)\n",
    "                plt.cla()\n",
    "                plt.scatter(samples.detach().numpy()[:,0],samples.detach().numpy()[:,1], alpha=0.3)\n",
    "                plt.grid()\n",
    "                \n",
    "                display.clear_output(wait=True)\n",
    "                display.display(plt.gcf())\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not bad, matches our input distribution. \n",
    "\n",
    "Remember the denoising diffusion model operates on a noise sample the same dimension of the data. In this case a 2d sample. It doesn't have to, and you could do this in a latent space too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy force fields\n",
    "\n",
    "One way to interpret the diffusion process is as a series of vector fields that push particles to lie on the data distribution manifold. Let's take a look at this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.linspace(-3, 3, steps=10)\n",
    "ys = torch.linspace(-3, 3, steps=10)\n",
    "x, y = torch.meshgrid(xs, ys, indexing='xy')\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "xs = ddpm.sample(model,n_samples=100,x=torch.hstack([x.reshape(-1,1),y.reshape(-1,1)]),plot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending to images\n",
    "\n",
    "Ok, this toy example just generates 2D points. Let's scale this up to generate full images! We'll replace the FCN with a unet and train a diffusion model to generate images using the CIFAR 10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear our GPU ram\n",
    "torch. cuda. empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "n_steps = 1000\n",
    "ddpm = DDPM(n_steps=n_steps).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull a unet backbone from this nice [repo](https://github.com/pesser/pytorch_diffusion)\n",
    "This is basically a standard unet, but with the addition of a time embedding so it takes arguments (im,t)\n",
    "\n",
    "## How safe is this?\n",
    "\n",
    "As we start to do more advanced things, we are relying on pre-existing models and code bases from seemingly random sources. We have historically had a *move fast and break things* culture in our field, but this is very problematic when your systems mature, and you start to run into issues. We often call this technical debt - bad or easy choices we make may come back to bite us down the line. For example:\n",
    "\n",
    "* Licensing issues - these models may be subject to licenses that prevent their use in certain applications, or require them to be used subject to certain conditions (eg. free to use, as long as you make all code using these open source). You may be required to pay for some code bases in commercial applications.\n",
    "* Security - can we trust that these models don't have malicious code embedded in them? \n",
    "* Ethical considerations - where did the data used to train these models come from, what is hidden inside them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -e git+https://github.com/pesser/pytorch_diffusion.git#egg=pytorch_diffusion\n",
    "\n",
    "from pytorch_diffusion import Model\n",
    "\n",
    "model = Model(resolution=32,\n",
    "              in_channels=3,\n",
    "              out_ch=3,\n",
    "              ch=128,\n",
    "              ch_mult=(1,2,2,2),\n",
    "              num_res_blocks=2,\n",
    "              attn_resolutions=(16,),\n",
    "              dropout=0.1).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optim = torch.optim.Adam(model.parameters(), 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "data = torchvision.datasets.CIFAR10(root='./data/',transform=transform)\n",
    "train_loader = DataLoader(data,batch_size=256,shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training this yourself will take some time (a couple days). Why is this so expensive? Look at the sampling step and count how many forward passes we needed to do to generate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = None  # exponential moving average\n",
    "    \n",
    "for step in range(n_epochs):\n",
    "\n",
    "    t = tqdm(total=len(train_loader))\n",
    "\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        im,_ = batch\n",
    "        \n",
    "        loss = ddpm.diffusion_loss(model, im.to(device),device=device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # Some visualisation during training.\n",
    "        t.update(1)\n",
    "        \n",
    "        if avg_loss is None:\n",
    "            avg_loss = loss.item()\n",
    "        else:\n",
    "            avg_loss = 0.95*avg_loss + 0.05*loss.item()\n",
    "        t.set_description(f\"Iter: {step}. Average Loss: {avg_loss:.04f}\")\n",
    "    t.reset()\n",
    "        \n",
    "    plt.clf()\n",
    "    plt.imshow(np.clip((ddpm.sample(model,x=torch.randn(1,3,32,32).to(device),n_samples=1,device=device).transpose(1,3).detach().cpu().squeeze()),0,1))  \n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = 25\n",
    "ims = np.clip(ddpm.sample(model,x=torch.randn(Ns,3,32,32).to(device),n_samples=Ns,device=device).detach().cpu(),0,1)\n",
    "grid_im = torchvision.utils.make_grid(ims, nrow=5).transpose(0,2)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(grid_im)\n",
    "plt.title('Generated images')\n",
    "grid_im = torchvision.utils.make_grid(im[0:25], nrow=5).transpose(0,2)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(grid_im)\n",
    "plt.title('Real images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that - so much nicer than the GAN and VAE!\n",
    "\n",
    "### Remember I made a promise in Week 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we make this faster and more computationally efficient? Well, we can play use some tricks in the noise schedule, we can play with the integration scheme used to rollout the noise field. \n",
    "\n",
    "Or, we can run the diffusion model in the latent space of an autoencoder, which is more efficient. If you combine a [VAE encoder + ViT + DDPM + VAE decoder](https://arxiv.org/abs/2212.09748) you get state of the art image generation. [OpenAI's SORA](https://openai.com/research/video-generation-models-as-world-simulators) video generation approach basically does this, but with the encoder acting on frame sequences. \n",
    "\n",
    "Interested in learning more - for a deep dive I recommend [Lilian Weng's blog](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
