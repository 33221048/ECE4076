{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc181fe-769d-45fe-ab8e-45c77d647bee",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> ECE4076/5176 - Week 12 </h1>\n",
    "<h1 align=\"center\"; style=\"color: purple;\"> Fun with OpenAI- Contrastive Language-Image Pretraining (CLIP) </h1>\n",
    "\n",
    "\n",
    "Throughout this unit, we have mostly only considered images as input, but what if we want to condition on other information. We've already had a taste of this when we built the conditional GAN, but we can apply this same strategy to any sort of data. Let's look at a cool approach that maps text and images together. CLIP learns to map text and images into representations that look similar, basically the model is trained to learn which text embeddings and images were paired together. The image below shows this objective, we will take a bunch of images, and train an image encoder to learn a representation of these. We will do the same with a text description. The loss we will use seeks to make some distance (eg. cosine) betweem these embeddings small for matching text and images, and large (contrastive) for unmatched pairs. Once we have this, we can do a lot of interesting things. \n",
    "\n",
    "![The clip model](https://images.openai.com/blob/fbc4f633-9ad4-4dc2-bd94-0b6f1feee22f/overview-a.svg)\n",
    "\n",
    "Let's see how we can do *unsupervised* object recognition using a pre-trained clip model. You need to have the following packages to work with this notebook\n",
    "- [pytorch](https://pytorch.org/)\n",
    "- [OpenCV](https://pypi.org/project/opencv-python/)\n",
    "- [numpy](https://anaconda.org/anaconda/numpy)\n",
    "- [matplotlib](https://anaconda.org/conda-forge/matplotlib)\n",
    "- [pillow](https://pillow.readthedocs.io/en/stable/)\n",
    "- [CLIP](https://github.com/openai/CLIP.git)\n",
    "\n",
    "To install CLIP, you need to run the following command\n",
    "!pip install git+https://github.com/openai/CLIP.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460fdcd0-02d1-43d8-a545-b5ef6470642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f6052-5357-4568-bb24-20c6cc81ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac799fa0-d1af-48c0-b0f1-ef73666832b3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<h1> CLIP</h1> \n",
    "</div>\n",
    "\n",
    "ClIP is a vision-language model. \n",
    "By matching text descriptions of images to their visual information, it learns associations between the language and vision.  Below, we check the vision models provided by OpenAI and opt for using a simple ResNet model (a CNN). CLIP comes with bigger and more powerful vision encoders such as Vision Transformer (ViT) (eg., ViT-B/32). Feel free to investigate those models as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5fdd1a-b770-4345-9203-e428d8bdbb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f03e6fd-d149-46d1-b84f-664015cb4f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"RN50\")\n",
    "model.to(device).eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea56b98-a2c3-4642-b3d1-e6de46404401",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Language Tokenizer  \n",
    "\n",
    "</div>\n",
    "\n",
    "To employ the CLIP, we need to process language. CLIP comes with a language model (LM). The first step in processing text data is to convert a sentence into tokens. Check out the code below, do you see any patterns? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf2247-3513-4486-9f6d-2620e3ad6ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_clip = clip.tokenize(\"Hello\").numpy()\n",
    "print(\"Tokenizing the word \\\"Hello\\\":\", token_clip[0,:4],\"\\n\")\n",
    "token_clip = clip.tokenize(\"Hello World\").numpy()\n",
    "print(\"Tokenizing the word \\\"Hello World\\\":\", token_clip[0,:4],\"\\n\")\n",
    "token_clip = clip.tokenize(\"Hello World!\").numpy()\n",
    "print(\"Tokenizing the word \\\"Hello World!\\\":\", token_clip[0,:4],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482883bc-30d0-4cf7-acd1-f9ee413fe5a1",
   "metadata": {},
   "source": [
    "As you might have guessed, the tokenizer uses some symbols to label words in a sequence. Every sequence starts with a Start-Of-Sequence (SoS) token and finishes with a End-of-Sequence (EoS) token. There is an Unknown token to describe every word that the CLIP language model does not understand. \n",
    "\n",
    "To have some fun, I am using the classes from the CIFAR100 to create a Zero-Shot Image Classifier. First, let's see what those classes are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb210cb-ecc6-4dc4-b5e5-1268111730a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZSL_classes = [\"beaver\", \"dolphin\", \"otter\", \"seal\", \"whale\",\n",
    "               \"aquarium fish\", \"flatfish\", \"ray\", \"shark\",\n",
    "               \"trout\", \"orchids\", \"poppies\", \"roses\", \n",
    "               \"sunflowers\", \"tulips\", \"bottles\", \"bowls\", \"cans\", \n",
    "               \"cups\", \"plates\", \"apples\", \"mushrooms\", \"oranges\", \n",
    "               \"pears\", \"sweet peppers\", \"clock\", \n",
    "               \"computer keyboard\", \"lamp\", \"telephone\", \"television\",\n",
    "               \"bed\", \"chair\", \"couch\", \"table\", \"wardrobe\",\n",
    "               \"bee\", \"beetle\", \"butterfly\", \"caterpillar\", \n",
    "               \"bear\", \"leopard\", \"lion\", \"tiger\", \"wolf\",\n",
    "               \"bridge\", \"castle\", \"house\", \"road\", \"skyscraper\",\n",
    "               \"cloud\", \"forest\", \"mountain\", \"plain\", \"sea\",\n",
    "               \"camel\", \"cattle\", \"chimpanzee\", \"elephant\", \"kangaroo\",\n",
    "               \"fox\", \"porcupine\", \"possum\", \"raccoon\", \"skunk\",\n",
    "               \"crab\", \"lobster\", \"snail\", \"spider\", \"worm\",\n",
    "               \"baby\", \"boy\", \"girl\", \"man\", \"woman\",\n",
    "               \"crocodile\", \"dinosaur\", \"lizard\", \"snake\", \"turtle\",\n",
    "               \"hamster\", \"mouse\", \"rabbit\", \"shrew\", \"squirrel\",\n",
    "               \"maple\", \"oak\", \"palm\", \"pine\", \"willow\",\n",
    "               \"bicycle\", \"bus\", \"motorcycle\", \"pickup truck\", \"train\",\n",
    "               \"lawn-mower\", \"rocket\", \"streetcar\", \"tank\", \"tractor\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddf8253-e6c5-4dd1-a43c-6f875bff84bd",
   "metadata": {},
   "source": [
    "Next, for each class, we create a prompt in the form \"This is a photo of [Class]\". Using prompts is shown to improve the quality of our inference so instead of describing class of \"cats\" by feeding the word cat to the model, we describe it by the prompt \"This is a photo of a cat\". The code below does this for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c239a-a171-4665-ae0e-74b206584bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_descriptions = [f\"This is a photo of a {label}\" for label in ZSL_classes]\n",
    "text_tokens = clip.tokenize(text_descriptions).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Text features\n",
    "    text_features = model.encode_text(text_tokens).float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "text_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9dac6-3b92-4230-af40-1dd2be6939bf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Zero-Shot Image Classification  \n",
    "\n",
    "</div>\n",
    "\n",
    "Each text feature above represent its corresponding class. With CLIP, now what we can do is to feed its vision model an image, and obtain the corresponding representation. Then, we need to compare the visual representation with our text representation to identify the predictions. Below, we start by reading an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79433c9-a491-48e3-8eca-8fbae9d1a9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://images.immediate.co.uk/production/volatile/sites/3/2019/05/EBC1840_v228.1047-eb60675.jpg\"\n",
    "\n",
    "pil_img = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "plt.imshow(pil_img)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b141cc4d-0ef2-47dd-9200-887351dd451f",
   "metadata": {},
   "source": [
    "Now, we feed the image to the CLIP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c4f07-84f3-4f97-8a5c-f8478d7e173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_clip = preprocess(pil_img).unsqueeze(dim=0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Image Features\n",
    "    image_features = model.encode_image(image_clip).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb1559-3ae5-4257-99c2-4e22467416ed",
   "metadata": {},
   "source": [
    "Below, we compare the image features with text features. This can be done by computing the cosine similarity between the vectors. Note that above, we normalize the text features. Since norm of the image features is not going to change our preditions (it simply scales the similarities), we do not bother with normalizing the image features. Code below, first computes the (scaled) cosine similarity using the inner product and turn the resulting prediction into a probability vector using a softmax function. We plot the top 5 predictions as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0848ef5-5f57-44dd-98e3-6595c839f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zsl_clip(image_features):\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)\n",
    "    top_probs = top_probs.detach().numpy().ravel()\n",
    "    top_labels = top_labels.detach().numpy().ravel()\n",
    "    return top_probs, top_labels\n",
    "\n",
    "def plot_clip_output(pil_img, top_probs,top_labels):\n",
    "    plt.subplot(1, 2,1)\n",
    "    plt.imshow(pil_img)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    y = np.arange(top_probs.shape[-1])\n",
    "    plt.grid()\n",
    "    plt.barh(y, top_probs)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_axisbelow(True)\n",
    "    plt.yticks(y, [ZSL_classes[index] for index in top_labels])\n",
    "    plt.xlabel(\"probability\")\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d3e3f-eb38-491d-80a1-3cb015f702a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_probs, top_labels = zsl_clip(image_features)\n",
    "plot_clip_output(pil_img, top_probs,top_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d0150b-c7d9-4dee-8ef1-239f5435984b",
   "metadata": {},
   "source": [
    "Maybe trying more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f511540-8768-43db-b4f9-b4280955c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://scikit-image.org/skimage-tutorials/_images/4_segmentation_15_0.png\"\n",
    "\n",
    "pil_img = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "image_clip = preprocess(pil_img).unsqueeze(dim=0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Image Features\n",
    "    image_features = model.encode_image(image_clip).float()\n",
    "top_probs, top_labels = zsl_clip(image_features)\n",
    "plot_clip_output(pil_img, top_probs,top_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7ae519",
   "metadata": {},
   "source": [
    "Nice, we just got an object recognition model with no training data! Although, technically we did have a lot of training data, because we relied on openai to collect a number of images and text correspondences, and then just repurposed their model for our application. Some people refer to this as a foundation model. Note that their is no model training in this paradigm - you just need to collect appropriate labels for your use case. But you are dependent on a third party company/ supplier to provide the model, and your system will only ever be as good as their training data and model. This may be problematic depending on your application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
