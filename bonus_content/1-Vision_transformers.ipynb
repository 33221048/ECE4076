{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vision Transformers\n",
    "\n",
    "This notebook will run through an example of a vision transformer. So far, we have seen how the convolution operation can be used to respond to find spatially invariant features in images, in effect acting like a learnable image filter. This is particularly powerful in learning the local texture and frequency changes that help us recognise or identify things in images.\n",
    "\n",
    "However, these operations are not great at bringing in global context. For example, if you wanted to predict the pose of a partially occluded human in an image, seeing the location of the head, an arm or a leg is really useful information or context to inform the location of the full body pose. CNNs use fully connected layers to try to pull in this global context, but we can be more explicit about it using attention, the key mechanism behind a transformer.\n",
    "\n",
    "Lets explore this with the CIFAR10 dataset, a set of small 32x32 images from one of 10 classes. Our goal will be to learn to predict the image class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets set up some transformations to provide some data augmentation\n",
    "transforms_train = transforms.Compose([\n",
    "\ttransforms.RandomCrop(32, padding=4),\n",
    "\ttransforms.RandomHorizontalFlip(),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# We won't used these for testing\n",
    "transforms_test = transforms.Compose([\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# We'll set up data loaders using pytorch\n",
    "cf_train = datasets.CIFAR10(root='./data/',train=True,download=True,transform=transforms_train)\n",
    "cf_test = datasets.CIFAR10(root='./data/',train=False,download=True,transform=transforms_test)\n",
    "\n",
    "# Let's choose some parameters for the model\n",
    "patch_size = 4 #I want my patches to be 4x4 pixels large\n",
    "num_patches = (32//patch_size)**2 # For 32x32 dim images, that means we have n patches \n",
    "batch_size = 128 # I will use a batch size of 128 to train my model\n",
    "\n",
    "trainloader = DataLoader(cf_train,batch_size=batch_size,shuffle=True,pin_memory=True,num_workers=8)\n",
    "testloader = DataLoader(cf_test,batch_size=batch_size,shuffle=True,pin_memory=True,num_workers=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at our data. Re-run the cell below a couple times to see a few random samples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check on data - it gives us some images, but they've been normalised so we can't really make out much in them\n",
    "ims,classes= next(iter(trainloader))\n",
    "plt.imshow(ims[0,:,:,:].transpose(0,2).transpose(0,1)*np.array([0.2023, 0.1994, 0.2010])+np.array([0.4914, 0.4822, 0.4465]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we will need for our vision transformer is a patch embedder. This will break our images into different patches, to help us better capitalise on the global context in the image to make decisions. The [original vision transformer](https://arxiv.org/pdf/2010.11929v2.pdf) breaks an image into 16x16 patches, but we chose a smaller number because our CIFAR10 images are quite small.\n",
    "\n",
    "*Dosovitskiy et. al, An image is worth 16x16 Words: Transformers for Image Recognition at Scale, ICLR 2021*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(32//patch_size):\n",
    "    for j in range(32//patch_size):\n",
    "        plt.subplot(32//patch_size,32//patch_size,32//patch_size*i+j+1)\n",
    "        # Note that we transpose here because torch expects channel x dim x dim images\n",
    "        plt.imshow(ims[0,:,patch_size*j:patch_size*(j+1),patch_size*i:patch_size*(i+1)].transpose(0,2).transpose(0,1)*np.array([0.2023, 0.1994, 0.2010])+np.array([0.4914, 0.4822, 0.4465])) \n",
    "        plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our patch embedder does this, and uses a linear projection to convert each patch into and embedding (feature) vector. Note that this removes the typical ***inductive bias*** associated with convolution - no knowledge of spatial pixel information ***within*** the patch is provided. If you want both, maybe you need to use a few fully convolutions on the entire image to extract this information first, before patch embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedder(nn.Module):\n",
    "    \n",
    "    def __init__(self,dim=768,patch_size=20):\n",
    "        super(PatchEmbedder,self).__init__()\n",
    "        \"\"\"\n",
    "        Split the word into patches using a strided convolution operation\n",
    "        This operation turns each patch into a 768 dimensional vector\n",
    "        \"\"\"\n",
    "        self.model = nn.Conv2d(3,dim,kernel_size=(patch_size,patch_size),stride=(patch_size,patch_size))\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        patches = self.model(x)\n",
    "        \n",
    "        return patches.flatten(2) #flatten all the patches into one long vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check on patch embedder - the size here should produce batch x d x # patches\n",
    "PatchEmbedder(patch_size=patch_size)(ims).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our patch embedding is going to be combined with a positional embedding - a unique, learnable code that is used to help the network keep track of which patch this embedding corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbedder(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim=768, num_patches=6,p=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pos_enc = nn.Parameter(torch.randn(1,num_patches,dim),requires_grad=True) # These positional encodings are learned nn.Parameter is trainable\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    def forward(self,x):\n",
    "        \n",
    "        return self.dropout(x + self.pos_enc) #Add input and positional encoding \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Sanity check on pos encoder - the size here should produce batch x # patches x d\n",
    "Note that we transpose our patch embeddings first before adding the positional encodings\n",
    "This will help us later with the attention mechanism we build\n",
    "'''\n",
    "PosEmbedder(num_patches=num_patches)(PatchEmbedder(patch_size=patch_size)(ims).transpose(1,2)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll implement the key part of our vision transformer, a self attention block. This takes an input and passes it through 3 separate linear layers to generate a query, key and value feature vector. The inner product between the query and key vectors is used to produce an *attention* matrix that passes key value information along in the network. \n",
    "\n",
    "Each attention block will perform this operation a few times, using multiple attention heads. We term this *multi-head attention*. With some clever reshaping, we can perform multi-head attention using a single operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, dim=768,dim_head=64,heads=8,p=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = heads\n",
    "        \n",
    "        self.out = nn.Sequential(nn.Linear(dim_head*heads,dim),\n",
    "                                nn.Dropout(p))\n",
    "            \n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "        self.attention = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # This is done with one operation, but could be separated\n",
    "        self.qkv = nn.Linear(dim,dim_head*heads*3,bias=False)\n",
    "            \n",
    "        self.scale = dim_head**-0.5\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        xn = self.norm(x)\n",
    "        \n",
    "        '''\n",
    "        We are implementing multi-head self attention so a few steps are needed here:\n",
    "        * Get query, key and value (batch x patches+1 x #heads*head dim*3) \n",
    "        * Reshape to batch x patches+1 x heads x head_dim*3\n",
    "        * Swap head and patch dims\n",
    "        * Split into query, key and value dims\n",
    "        * Each embedding is now batch x heads x patches+1 x head_dim                                  \n",
    "        '''                            \n",
    "        \n",
    "        q,k,v = self.qkv(xn).reshape(x.shape[0],x.shape[1],self.heads,-1).transpose(1,2).chunk(3,dim=-1)\n",
    "        \n",
    "        '''\n",
    "        We'll implement scale dot product attention:\n",
    "        * Compute inner product between query and k and scale\n",
    "        * Attention matrix is now batch x heads x patches+1 x patches+1\n",
    "        '''\n",
    "        attn = (self.dropout(self.attention(q@k.transpose(-2,-1)*self.scale)))\n",
    "        \n",
    "        '''\n",
    "        Compute attention score for each head\n",
    "        * Score is batch x heads x patches+1 x head_dim\n",
    "        * Swap heads and patch dim\n",
    "        * Collapse heads to single embedding, batch x patches+1 x heads*head_dim\n",
    "        \n",
    "        '''\n",
    "        score = (attn@v).transpose(1,2).reshape(x.shape[0],x.shape[1],-1)\n",
    "        \n",
    "        # Pass through linear layer to restore to hidden dim \n",
    "        return self.out(score)\n",
    "    \n",
    "    def get_attention(self,x):\n",
    "        \n",
    "        xn = self.norm(x)\n",
    "        q,k,v = self.qkv(xn).reshape(x.shape[0],x.shape[1],self.heads,-1).transpose(1,2).chunk(3,dim=-1)\n",
    "        attn = (self.dropout(self.attention(q@k.transpose(-2,-1)*self.scale)))\n",
    "        score = (attn@v).transpose(1,2).reshape(x.shape[0],x.shape[1],-1)\n",
    "        \n",
    "        # Pass through linear layer to restore to hidden dim \n",
    "        return self.out(score), attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our attention block will be paired with an MLP head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim=768,hidden_dim=64,p=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(nn.LayerNorm(dim),\n",
    "                                 nn.Linear(dim,hidden_dim),\n",
    "                                 nn.GELU(),\n",
    "                                 nn.Dropout(p),\n",
    "                                 nn.Linear(hidden_dim,dim),\n",
    "                                 nn.GELU(),\n",
    "                                 nn.Dropout(p))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine these blocks, adding layer normalisation and skip connections, to produce a *Transformer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim=768,dim_head=64,heads=8,hidden_dim=128,p=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nromalises over last dim dimensions\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        \n",
    "        self.attn = AttentionBlock(dim=dim,heads=heads,dim_head=dim_head,p=p)\n",
    "        self.ff = MLP(dim=dim,hidden_dim=hidden_dim,p=p)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        # skip connections\n",
    "        x = self.attn(x) + x\n",
    "        x = self.ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n",
    "    \n",
    "    def get_attention(self,x):\n",
    "        \n",
    "        # skip connections\n",
    "        xo,amap = self.attn.get_attention(x)\n",
    "        \n",
    "        x = xo + x\n",
    "        x = self.ff(x) + x\n",
    "\n",
    "        return self.norm(x), amap\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's use these to build our Vision Transformer. This will positionally encode a patch embedder, then pass it through a series of Transformers before moving to a classification layer. There is one more tweak though. To aid classification, we will add a token to the list of patches that will be used for classification. So our transformer will be trained to link patches to this token to make classification decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self,dim=768,patch_size=16,n_classes=6,num_patches=6,layers=1,heads=8,dim_heads=64,hidden_dim=128,p=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patchifier = PatchEmbedder(dim=dim,patch_size=patch_size)\n",
    "        self.position_encoder = PosEmbedder(dim=dim,num_patches=num_patches+1)\n",
    "        tf_list = nn.ModuleList([Transformer(dim=dim,dim_head=dim_heads,heads=heads,hidden_dim=hidden_dim,p=p) for n in range(layers)])\n",
    "        self.transformer = nn.Sequential(*tf_list)\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "        # Add a trainable token for classification\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,dim))\n",
    "        \n",
    "        self.classifier = nn.Linear(dim,n_classes)\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self,im):\n",
    "        \n",
    "        patch_embeddings = self.patchifier(im)\n",
    "        \n",
    "        # Extend the patch embeddings to add the class token to the patches\n",
    "        v = torch.cat((self.cls_token.expand(patch_embeddings.shape[0],-1,-1).transpose(1,2),patch_embeddings),dim=-1)\n",
    "        # Positionally encode\n",
    "        pos_encodings = self.position_encoder(v.transpose(1,2))\n",
    "        # Transformer\n",
    "        x = self.transformer(self.dropout(pos_encodings))\n",
    "        # Return the classifier decision made using our class token [bin 0]\n",
    "        return self.classifier(x[:,0,:])\n",
    "    \n",
    "    def show_attention(self,im):\n",
    "        \n",
    "        patch_embeddings = self.patchifier(im)\n",
    "        \n",
    "        # Extend the patch embeddings to add the class token to the patches\n",
    "        v = torch.cat((self.cls_token.expand(patch_embeddings.shape[0],-1,-1).transpose(1,2),patch_embeddings),dim=-1)\n",
    "        # Positionally encode\n",
    "        x = self.position_encoder(v.transpose(1,2))\n",
    "        \n",
    "        att_list = []\n",
    "        for i in range(self.layers):\n",
    "            x,attn = self.transformer[i].get_attention(self.dropout(x))\n",
    "            att_list.append(attn)\n",
    "        \n",
    "        return att_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup and train our vision transformer on the CIFAR10 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our model and training details\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "layers = 2\n",
    "heads = 6\n",
    "dim = 384\n",
    "\n",
    "\n",
    "VIT = VisionTransformer(dim=dim,patch_size=patch_size,n_classes=10,heads=heads,num_patches=num_patches,layers=layers,hidden_dim=dim,p=0.5).to(device)\n",
    "opt = torch.optim.AdamW(VIT.parameters(),lr=1e-4,weight_decay=1e-5)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()# Use a cross entropy classification loss\n",
    "\n",
    "Nepochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be quite slow without a GPU. While it trains we will visualise the attention maps our model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "plt.figure(figsize=(15,5))\n",
    "for epoch in range(Nepochs):\n",
    "    \n",
    "    VIT.train() # Make model trainable\n",
    "    \n",
    "    losses = []\n",
    "    total_len = 0\n",
    "    correct = 0\n",
    "    # Iterate over batches in training set\n",
    "    for ims,counts in trainloader:\n",
    "        \n",
    "        predictions = VIT(ims.to(device))\n",
    "        \n",
    "        opt.zero_grad() # zero previous gradients\n",
    "        \n",
    "        loss = loss_fn(predictions,counts.to(device))\n",
    "        \n",
    "        loss.backward() # compute gradient of model parameters with respect to output\n",
    "        \n",
    "        correct += (nn.functional.softmax(predictions,dim=-1).argmax(dim=-1)==counts.to(device)).sum().cpu()\n",
    "        total_len += counts.shape[0]    \n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(VIT.parameters(), 0.1)\n",
    "        \n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    VIT.eval() # Put model in eval mode\n",
    "    \n",
    "    # Lets visualise our learned attention maps\n",
    "    plt.clf()\n",
    "   \n",
    "    plt.subplot(1,3,1)\n",
    "    # Un-normalise the images to make it easier to visualise\n",
    "    plt.imshow(ims[0,:,:].transpose(0,2).transpose(0,1)*np.array([0.2023, 0.1994, 0.2010])+np.array([0.4914, 0.4822, 0.4465]))\n",
    "    # Extract attention maps, there will be layers x heads maps\n",
    "    att_list = VIT.show_attention(ims.to(device)) \n",
    "    ave_attention = np.zeros((32//patch_size,32//patch_size))\n",
    "    for i in range(len(att_list)):\n",
    "        for j in range(att_list[i].shape[1]):\n",
    "            plt.subplot(layers,3*heads,(3*i+2)*heads+j+1)   \n",
    "            '''\n",
    "            Some complicated indexing here. \n",
    "            Each attention map is batch x heads x n_patches+1 x n_patches+1\n",
    "            The +1 was for the classifier token\n",
    "            We will select a head, and the attention the classifier token paid to the patches\n",
    "            We will reshape that to an n patches x n patches saliency map\n",
    "            ''' \n",
    "            att = att_list[i][0,j,0,1:].reshape(32//patch_size,32//patch_size).detach().cpu().numpy()\n",
    "            \n",
    "            ave_attention += att\n",
    "            plt.imshow(att)\n",
    "            plt.title('Head %d %d'%(i,j))\n",
    "            plt.axis('off')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(ave_attention/(heads*layers))\n",
    "    plt.title('Average attention')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "    # Log some data\n",
    "    print(\"Epoch %d: Loss: %2.2f\"%(epoch,np.mean(losses)))\n",
    "    print(\"Epoch %d: Train Accuracy: %2.2f\"%(epoch,correct/total_len))\n",
    "    \n",
    "    # Check test set performance\n",
    "    correct = 0\n",
    "    total_len = 0\n",
    "    for ims, counts in testloader:\n",
    "        total_len += counts.shape[0]    \n",
    "        predictions = nn.functional.softmax(VIT(ims.to(device)),dim=-1)\n",
    "        correct += (predictions.argmax(dim=-1)==counts.to(device)).sum().cpu()\n",
    "    print(\"Epoch %d: Validation Accuracy: %2.2f\"%(epoch,correct/total_len))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "\n",
    "Hmmm, accuracy is not amazing... \n",
    "\n",
    "* Try make the model deeper, increase the heads and embedding dimensionality. \n",
    "* What happens to model performance? \n",
    "* Transformers work best when pre-trained on extremely large datasets, before being fine tuned. Why do you think this is? \n",
    "\n",
    "*Hint*: remember what we said about inductive biases above, and think about the engineering tradeoffs of structure vs data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
