{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Filtering\n",
    "\n",
    "In our previous notebook, we saw that we need computer vision algorithms that can extract information from information hidden in spatially varying signals, as most of the image information lay in the frequency, not amplitude (or intensity) of the image pixels. \n",
    "\n",
    "In our lectures this week, we introduced a filtering view to images, using the convolution operation. You'll implement this operation yourselves in a lab, but today we are going to try out some useful hand-designed filters. Later on in this unit, you will explore convolutional neural networks to perform more complex operations like image classification and object detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll load in my own test image, swap this for one of your own\n",
    "im = cv2.imread('../test_images/Drone_1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The convolution operation\n",
    "First, lets visualise the convolution operation using some inline plotting code. Lets imagine a square filter 200 pixels x 200 pixels sliding over our image. To make it simpler, I will make it jump ahead a few pixels at a time. We call this a strided convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rectangle(i,j,w):\n",
    "    plt.plot([i-w,i-w],[j-w,j+w],'k--')\n",
    "    plt.plot([i+w,i+w],[j-w,j+w],'k--')\n",
    "    plt.plot([i-w,i+w],[j-w,j-w],'k--')\n",
    "    plt.plot([i-w,i+w],[j+w,j+w],'k--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 200\n",
    "for row in np.linspace(0,im.shape[0],10,dtype=int):\n",
    "    for col in np.linspace(0,im.shape[0],10,dtype=int):\n",
    "        plt.clf() # clear the figure\n",
    "        plt.imshow(im[:,:,[2,1,0]])\n",
    "        plot_rectangle(col,row,w)\n",
    "        \n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution operation slides a mask, kernel or filter over the image, multiplies it with the pixel values under the mask, and sums these to produce an output response at a pixel location. \n",
    "\n",
    "We can immediately see a challenge you'll encounter when you implement your own convolution - how do we handle pixels outside the image? For the remainder of this lecture though, we'll use a built in convolution function from opencv. Lets start by implementing the averaging filter we saw in the lectures.\n",
    "\n",
    "To make things easier - we'll convert our image to a single channel grayscale image. I will also resize the image to make it a little smaller.\n",
    "\n",
    "#### Some homework - think about how you could resize an image using a convolution operation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_gray = cv2.cvtColor(cv2.resize(im,(400,400)), cv2.COLOR_BGR2GRAY)\n",
    " # Add some salt and pepper noise (missing values to 1 % of pixels)\n",
    "im_gray_noisy = im_gray*(np.random.rand(im_gray.shape[0],im_gray.shape[1])>0.01)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(im_gray,cmap='gray') # see how my even resizing has distorted the image a bit?\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(im_gray_noisy,cmap='gray') # see how my even resizing has distorted the image a bit?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to clean this up with an averaging filter, 3x3 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 1/9*np.ones((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technically this is correlation, not convolution, but it doesn't matter with a symmetric kernel\n",
    "response = cv2.filter2D(src=im_gray_noisy,ddepth=0,kernel=kernel)\n",
    "\n",
    "plt.imshow(response,cmap='gray')\n",
    "plt.title('Mean filter 3x3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, not great those missing pixels are still visible. Our image is also a bit blurry now. Lets try a larger filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 1/(15*15)*np.ones((15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cv2.filter2D(src=im_gray_noisy,ddepth=0,kernel=kernel)\n",
    "\n",
    "plt.imshow(response,cmap='gray')\n",
    "plt.title('Mean filter 9x9')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the noise is better, but the image looks quite blurry now. Lets try a median filter - we'll replace each pixel, by the most common in a set of blocks surrounding it. Unfortunately, this can't be represented that nicely using a convolution operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cv2.medianBlur(im_gray_noisy,ksize=3)\n",
    "\n",
    "plt.imshow(response,cmap='gray')\n",
    "plt.title('Median filter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks better, although if you look closely you will find there are some interesting artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1 \n",
    "\n",
    "Modify the code above to replace the noise with thermal noise. Re-run the code and compare the median and mean filter approaches. Hint thermal noise looks like additive gaussian noise - np.random.randn() can be used to generate normally distributed noise\n",
    "\n",
    "- Did the output change?\n",
    "- What type of approach is best suited to the different types of noise?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining filtering operations\n",
    "\n",
    "Our averaging filter used a single kernel and convolution operation, but we can also chain a few of this together to produce more interesting responses. Let's see how we can use two Gaussian filters to build an edge detector using a difference of Gaussians approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gaussian_kernel(d=15,sigma=3):\n",
    "    xx,yy = np.meshgrid(np.arange(d),np.arange(d))\n",
    "    f = 1.0/np.sqrt((2*np.pi)**2*sigma**2*sigma**2)*np.exp(-0.5*((xx-d/2)**2 + (yy-d/2)**2)/sigma**2)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let start by building a 15x15 Gaussian kernel, with a standard deviation of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = make_gaussian_kernel(15,3)\n",
    "plt.imshow(kernel,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_response = cv2.filter2D(src=im_gray,ddepth=-1,kernel=kernel)\n",
    "\n",
    "plt.imshow(gaussian_response,cmap='gray')\n",
    "plt.title('Gaussian blur 15x15 $\\sigma=3$')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some nice soft focus there. Lets convolve with a smaller standard deviation now, and subtract the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_1 = make_gaussian_kernel(15,1)\n",
    "kernel_3 = make_gaussian_kernel(15,3)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(kernel_1,cmap='gray')\n",
    "plt.title(\"Kernel $\\sigma=1$\")\n",
    "plt.colorbar()\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(kernel_3,cmap='gray')\n",
    "plt.title(\"Kernel $\\sigma=3$\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_response_1 = cv2.filter2D(src=im_gray,ddepth=-1,kernel=kernel_1)\n",
    "gaussian_response_2 = cv2.filter2D(src=im_gray,ddepth=-1,kernel=kernel_3)\n",
    "\n",
    "dog = gaussian_response_1-gaussian_response_2\n",
    "\n",
    "plt.imshow(dog,cmap='gray')\n",
    "plt.title('Difference of Gaussians')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we built an edge detector, using basic maths and two convolutional filters. We can start to build filters up to form more complicated responses, that extract targeted information. Later in this unit, you will see that convolutional neural networks and deep learning simply use multiple chained layers of these simple operations with learned filters to extract useful information from images. You'll also see a similar approach later in the unit when you use Haar wavelets to extract information for face detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2 - Image sharpening\n",
    "\n",
    "Write a small code snippet to add the edge response above to the original image and visualise the result. \n",
    "\n",
    "- What effect does this have?\n",
    "- Does this create any unusual artifacts in your image?\n",
    "- What happends to the texture in the background?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_image = #Add your code here\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(result_image,cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.title('Sharpened image')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(im_gray,cmap='gray')\n",
    "plt.title('Original image')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keypoint detection and patch matching\n",
    "\n",
    "In the labs, you will implement some Sobel filters for edge detection and corner detection. Keypoints are a useful representation that reduce the dimensionality of an image, while still retaining a lot of information. We'll use these a lot for homography estimation and 3D reconstruction approaches, and see some examples in panaromas and be useful in augmented reality applications. Lets use OpenCV to find some Harris corners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harris_response = cv2.cornerHarris(im_gray,blockSize=2,ksize=3,k=0.04)\n",
    "# Pick all the corners greater than 1 % of the strongest corner response\n",
    "corners_thresholded = harris_response>0.05*harris_response.max() \n",
    "\n",
    "corners_x,corners_y = np.where(corners_thresholded) # coordinates of corners\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(im_gray,cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(corners_thresholded,cmap='gray')\n",
    "plt.title('Harris corners')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load in another image of the same drone, taken from a slightly different viewpoint\n",
    "im_gray_2 = cv2.cvtColor(cv2.resize(cv2.imread('../test_images/Drone_2.jpg'),(400,400)), cv2.COLOR_BGR2GRAY)\n",
    "harris_response_2 = cv2.cornerHarris(im_gray_2,blockSize=2,ksize=3,k=0.04)\n",
    "# Pick all the corners greater than 1 % of the strongest corner response\n",
    "corners_thresholded_2 = harris_response_2>0.05*harris_response.max() \n",
    "\n",
    "corners_x_2,corners_y_2 = np.where(corners_thresholded_2) # coordinates of corners\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(im_gray_2,cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(corners_thresholded_2,cmap='gray')\n",
    "plt.title('Harris corners')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets crop a 80x80 patch around the 50th corner in the second image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch(im,x,y,w=10):\n",
    "    \n",
    "    patch = im[x-w:x+w,y-w:y+w]\n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_x = corners_x_2[50]\n",
    "pos_y = corners_y_2[50]\n",
    "test_patch = get_patch(im_gray_2,pos_x,pos_y)\n",
    "plt.imshow(test_patch,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to search for the most similar patch in the original image. We'll use an ssd distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssd(patch_1,patch_2):\n",
    "    \n",
    "    if (patch_1.shape != patch_2.shape): #Ignore patches around the edges \n",
    "        return 1e15\n",
    "    else:\n",
    "        return np.sum((patch_1-patch_2)**2)\n",
    "\n",
    "def find_closest_patch(im_to_search, kp_x_to_search, kp_y_to_search, query_patch):\n",
    "    \n",
    "    min_dist = 1e15\n",
    "    min_dist_idx = 0\n",
    "    # Loop over all corners\n",
    "    for i in range(len(kp_x_to_search)):\n",
    "        \n",
    "        pos_x = kp_x_to_search[i]\n",
    "        pos_y = kp_y_to_search[i]\n",
    "        test_patch = get_patch(im_to_search,pos_x,pos_y,w=int(query_patch.shape[0]/2))\n",
    "        \n",
    "        dist = ssd(test_patch,query_patch)\n",
    "        \n",
    "        if dist <= min_dist:\n",
    "            min_dist = dist\n",
    "            min_dist_idx = i\n",
    "            \n",
    "    pos_x = kp_x_to_search[min_dist_idx]\n",
    "    pos_y = kp_y_to_search[min_dist_idx]\n",
    "        \n",
    "    return get_patch(im_to_search,pos_x,pos_y), min_dist_idx    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_patch, patch_idx = find_closest_patch(im_gray,corners_x,corners_y,test_patch)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_patch,cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(best_patch,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, that looks like the same part of the image. Let's see how it does across all possible keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all keypoints in image 2, and find closest match in image 1\n",
    "match_idxs = []\n",
    "for j in range(len(corners_x_2)):\n",
    "    pos_x = corners_x_2[j]\n",
    "    pos_y = corners_y_2[j]\n",
    "    test_patch = get_patch(im_gray_2,pos_x,pos_y)\n",
    "    best_patch, patch_idx = find_closest_patch(im_gray,corners_x,corners_y,test_patch)\n",
    "    match_idxs.append(patch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matches(kpx_1,kpy_1,kpx_2,kpy_2,jump=100):\n",
    "    plt.plot([kpy_1,kpy_2+jump],[kpx_1,kpx_2],'b',alpha=0.2)\n",
    "\n",
    "im_both = np.hstack((im_gray_2,im_gray)) # Concatenate images\n",
    "plt.imshow(im_both,cmap='gray')\n",
    "for j in range(len(match_idxs)):\n",
    "    kpx_1 = corners_x_2[j]\n",
    "    kpy_1 = corners_y_2[j]\n",
    "    \n",
    "    kpx_2 = corners_x[match_idxs[j]]\n",
    "    kpy_2 = corners_y[match_idxs[j]]\n",
    "    plot_matches(kpx_1,kpy_1,kpx_2,kpy_2,jump=im_gray.shape[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 3: Image Invariances\n",
    "\n",
    "Hmm, look like there are some ok looking matches, but quite a few incorrect ones.\n",
    "\n",
    "Why do you think this is? Think about the sum of squared distances error and why it may fail to measure the similarity between image patches.\n",
    "\n",
    "Try to list some causes of variation that could influence this matching."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
