{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc181fe-769d-45fe-ab8e-45c77d647bee",
   "metadata": {
    "id": "bdc181fe-769d-45fe-ab8e-45c77d647bee"
   },
   "source": [
    "<div style=\"text-align:center\"><img src=\"https://www.dropbox.com/s/roovx0dx8mpm4ep/ECE4076_banner.png?dl=1\"></div>\n",
    "\n",
    "<h1 align=\"center\"> ECE4076/5176 - Week 11 </h1>\n",
    "<h1 align=\"center\"; style=\"color: purple;\"> Fun with Region-Based CNN (RCNN) </h1>\n",
    "\n",
    "\n",
    "\n",
    "You need to have the following packages to work with this notebook\n",
    "- [pytorch](https://pytorch.org/)\n",
    "- [OpenCV](https://pypi.org/project/opencv-python/)\n",
    "- [numpy](https://anaconda.org/anaconda/numpy)\n",
    "- [matplotlib](https://anaconda.org/conda-forge/matplotlib)\n",
    "- [pillow](https://pillow.readthedocs.io/en/stable/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460fdcd0-02d1-43d8-a545-b5ef6470642c",
   "metadata": {
    "id": "460fdcd0-02d1-43d8-a545-b5ef6470642c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import cv2\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55e8a7",
   "metadata": {
    "id": "bb55e8a7"
   },
   "outputs": [],
   "source": [
    "# the following function reads an image from a URL and displays it.\n",
    "def read_image_from_url(url):\n",
    "    img = cv2.resize(imageio.imread(url),(400,400))\n",
    "\n",
    "    return np.array(img)[:,:,[2,1,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf0159",
   "metadata": {
    "id": "2fdf0159"
   },
   "source": [
    "# Selective Search for Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa1bccb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aaa1bccb",
    "outputId": "a26aa896-b931-4ae3-9342-0396ec556e70"
   },
   "outputs": [],
   "source": [
    "# Load the image\n",
    "url = 'http://cdn.cnn.com/cnnnext/dam/assets/200130092551-02-market-street-san-francisco-car-free-now.jpg'\n",
    "image = read_image_from_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda785f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "eda785f3",
    "outputId": "743b254e-665f-4204-8e3b-88a852adf1bf"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert the image to Lab color space\n",
    "lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n",
    "\n",
    "# Create the superpixel object\n",
    "region_size = 20\n",
    "\n",
    "# This may need you to install the package opencv-conrib-python\n",
    "superpixel = cv2.ximgproc.createSuperpixelSLIC(lab_image, algorithm=cv2.ximgproc.SLICO, region_size=region_size)\n",
    "\n",
    "# Perform the superpixel segmentation\n",
    "superpixel.iterate()\n",
    "\n",
    "# Get the labels of the superpixels\n",
    "labels = superpixel.getLabels()\n",
    "\n",
    "# Create a mask for the superpixel boundaries\n",
    "mask = superpixel.getLabelContourMask()\n",
    "\n",
    "# Apply the mask on the original image to show the superpixel boundaries\n",
    "image_with_boundaries = cv2.bitwise_and(image, image, mask=mask)\n",
    "image_with_overlay = image.copy()\n",
    "\n",
    "# Apply an alpha blend effect by merging the image with boundaries using the mask\n",
    "alpha = 0.5  # Adjust the alpha value for desired blending effect\n",
    "image_with_overlay = cv2.addWeighted(image, 1-alpha, image_with_boundaries, alpha, 0)\n",
    "\n",
    "\n",
    "\n",
    "# Display the results\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Display the original image\n",
    "axs[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "axs[0].set_title(\"Original Image\")\n",
    "axs[0].axis(\"off\")\n",
    "\n",
    "# Display the image with superpixel\n",
    "axs[1].imshow(cv2.cvtColor(image_with_overlay, cv2.COLOR_BGR2RGB))\n",
    "axs[1].set_title(\"Superpixels\")\n",
    "axs[1].axis(\"off\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a07ea7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "53a07ea7",
    "outputId": "81c33b60-4bc0-4598-9509-a935cce75499"
   },
   "outputs": [],
   "source": [
    "# Create a Selective Search segmentation object\n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "# Set the input image for selective search\n",
    "ss.setBaseImage(image)\n",
    "\n",
    "# Perform selective search\n",
    "ss.switchToSelectiveSearchFast()  # Use fast mode for efficiency\n",
    "proposals = ss.process()\n",
    "\n",
    "image_with_proposals = image.copy()\n",
    "# Show the top 15 region proposals\n",
    "num_proposals = 15\n",
    "colors = np.random.randint(0, 255, (num_proposals, 3), dtype=np.uint8)\n",
    "for i, proposal in enumerate(proposals[-num_proposals:]):\n",
    "    x, y, w, h = proposal\n",
    "    color = tuple(map(int, colors[i]))\n",
    "    cv2.rectangle(image_with_proposals, (x, y), (x+w, y+h), color, 2)\n",
    "\n",
    "# Display the image with object proposals\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Display the original image\n",
    "axs[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "axs[0].set_title(\"Original Image\")\n",
    "axs[0].axis(\"off\")\n",
    "\n",
    "# Display the image with superpixel boundaries\n",
    "axs[1].imshow(cv2.cvtColor(image_with_proposals, cv2.COLOR_BGR2RGB))\n",
    "axs[1].set_title(\"Proposals\")\n",
    "axs[1].axis(\"off\")\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f6052-5357-4568-bb24-20c6cc81ed2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d1f6052-5357-4568-bb24-20c6cc81ed2d",
    "outputId": "4816f2c8-c4e9-4958-f77b-0ef26d51e740"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403f9c4a-4822-4100-b6bf-e33981ea6e26",
   "metadata": {
    "id": "403f9c4a-4822-4100-b6bf-e33981ea6e26"
   },
   "outputs": [],
   "source": [
    "COCO_INSTANCE_CATEGORY_NAMES = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "                                'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "                                'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "                                'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "                                'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "                                'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "                                'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "                                'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "                                'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "                                'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "                                'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "                                'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "                                ]\n",
    "\n",
    "\n",
    "det_COLORS = np.random.uniform(0, 255, size=(len(COCO_INSTANCE_CATEGORY_NAMES), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe59b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85fe59b6",
    "outputId": "d062b275-ecec-4101-b56e-b3d1af334d01"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import detection\n",
    "\n",
    "# load the model and set it to evaluation mode\n",
    "model = detection.fasterrcnn_resnet50_fpn(pretrained=True).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a5881d",
   "metadata": {
    "id": "d4a5881d"
   },
   "outputs": [],
   "source": [
    "# the following function reads an image from a URL and displays it.\n",
    "def read_image_from_url(url):\n",
    "    pil_img = Image.open(requests.get(url, stream=True).raw)\n",
    "    pil_img.thumbnail((800, 600), Image.ANTIALIAS)\n",
    "    #img_thumbnail = img.thumbnail((256,256), Image.ANTIALIAS)\n",
    "    plt.imshow(pil_img)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    img = np.array(pil_img)\n",
    "\n",
    "    torch_img = prepare_img_torch(img)\n",
    "    return torch_img, img\n",
    "\n",
    "\n",
    "\n",
    "def prepare_img_torch(img):\n",
    "    img = img/255.0\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    # add the batch dimension, scale the raw pixel intensities to the\n",
    "    # range [0, 1], and convert the image to a floating point tensor\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    torch_img = torch.FloatTensor(img)\n",
    "    # send the input to the device and pass the it through the network to\n",
    "    # get the detections and predictions\n",
    "    torch_img = torch_img.to(device)\n",
    "    return torch_img\n",
    "\n",
    "\n",
    "def draw_text(img, text,\n",
    "          font=cv2.FONT_HERSHEY_PLAIN,\n",
    "          pos=(0, 0),\n",
    "          font_scale=3,\n",
    "          font_thickness=2,\n",
    "          text_color=(255, 255, 255),\n",
    "          text_color_bg=(0, 0, 0)\n",
    "          ):\n",
    "\n",
    "    x, y = pos\n",
    "    text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
    "    text_w, text_h = text_size\n",
    "    cv2.rectangle(img, pos, (x + text_w, y + text_h), text_color_bg, -1)\n",
    "    cv2.putText(img, text, (x, y + text_h + font_scale - 1), font, font_scale, text_color, font_thickness)\n",
    "\n",
    "    return text_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53cd6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "id": "0f53cd6c",
    "outputId": "7f141a81-0313-40f8-99db-7868c21db35d"
   },
   "outputs": [],
   "source": [
    "url = \"http://cdn.cnn.com/cnnnext/dam/assets/200130092551-02-market-street-san-francisco-car-free-now.jpg\"\n",
    "# url = \"https://m.media-amazon.com/images/I/71xybHPToQL._AC_SL1500_.jpg\"\n",
    "# url = \"https://www.monash.edu/__data/assets/image/0006/959496/clayton-campus-green-chemical-futures-building-exterior2017.jpg\"\n",
    "\n",
    "img, img_rgb = read_image_from_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bac63f",
   "metadata": {
    "id": "83bac63f"
   },
   "outputs": [],
   "source": [
    "predictions = model(img)[0]\n",
    "\n",
    "pred_scores = predictions[\"scores\"].detach().cpu().numpy()\n",
    "pred_classes = predictions[\"labels\"].detach().cpu().numpy()\n",
    "pred_boxes = predictions[\"boxes\"].detach().cpu().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f90c91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46f90c91",
    "outputId": "5118202e-08e6-4276-d1d5-116fb809ad08"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(pred_boxes, columns = [\"x\",\"y\",\"w\",\"h\"])\n",
    "df[\"Confidence Scores\"] = pred_scores\n",
    "# df[\"Classes\"] = [obj_class for obj_class in COCO_INSTANCE_CATEGORY_NAMES[pred_classes]\n",
    "df[\"Classes\"] = [COCO_INSTANCE_CATEGORY_NAMES[idx] for idx in pred_classes]\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7677bc5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7677bc5a",
    "outputId": "0cb4f848-023f-4bf8-c411-5ba04491f1d2"
   },
   "outputs": [],
   "source": [
    "plt_img = img_rgb.copy()\n",
    "conf_threshold = 0.3\n",
    "\n",
    "\n",
    "# loop over the detections\n",
    "for confidence, idx, bb in zip(pred_scores, pred_classes, pred_boxes):\n",
    "\t# print(confidence, COCO_INSTANCE_CATEGORY_NAMES[idx])\n",
    "\n",
    "\t# filter out weak detections by ensuring the confidence is\n",
    "\t# greater than the minimum confidence\n",
    "\tif confidence > conf_threshold:\n",
    "\t\t# extract the index of the class label from the detections,\n",
    "\t\t# then compute the (x, y)-coordinates of the bounding box\n",
    "\t\t# for the object\n",
    "\t\tobj_class = COCO_INSTANCE_CATEGORY_NAMES[idx]\n",
    "\t\t(startX, startY, endX, endY) = bb.astype(\"int\")\n",
    "\t\t# display the prediction to our terminal\n",
    "\t\tprint(f\"An object from the class {obj_class} is found with a confidence score of {(confidence*100):.2f}\")\n",
    "\t\t# draw the bounding box and label on the image\n",
    "\t\tcv2.rectangle(plt_img, (startX, startY), (endX, endY),\tdet_COLORS[idx], 2)\n",
    "\t\ty = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "\t\t# cv2.putText(plt_img, obj_class, (startX, y),\tcv2.FONT_HERSHEY_PLAIN, 2, det_COLORS[idx], 2)\n",
    "\t\t# cv2.putText(plt_img, obj_class, (startX, y),\tcv2.FONT_HERSHEY_PLAIN, 2, detect_colors(idx), 2)\n",
    "\t\tdraw_text(plt_img, obj_class, pos=(startX, y),\ttext_color_bg=det_COLORS[idx])\n",
    "# show the output image\n",
    "plt.imshow(plt_img)\n",
    "\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
